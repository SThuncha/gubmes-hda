# Introduction to Machine Learning

Many of you might have heard of machine learning before, some of you might have even used it! Here we will start with the basics. First we need to know what machine learning is and WHY would we want to use it in Data Science.

### Task 1: 

There are 2 main types of machine learning tasks; supervirsed learning and unsupervirsed learning. Research what the main differences between these and what tasks they might be useful for.

```
1. supervised learning - The input data is labeled. 
                       - The data is classifly based on the training dataset.
                       - Used for prediction.
                       - Has a feedback mechanism.
                       - A known number of classes.
2. unsupervised learning - The iput data is unlabeled. 
                         - Assigns properties of given data to classify it.
                         - Used for analysis.
                         - Has no feedback mechanism.
                         - A unnown number of classes.
```

To start us of with we will be using a supervirsed machine learning method. One of simplest methods is the k-Nearest Neighbors. 


### Task 2

Research the k-Nearest Neighbors algortihtm and how it can be used for classification and regression models, detail your findings below. 

Useful links to start you off with:
[Neighbors Classification](https://scikit-learn.org/stable/modules/neighbors.html#classification) , 
[Plotting](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py).

```
  Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.
  
  scikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user. RadiusNeighborsClassifier implements learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value specified by the user.
  
  Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.

  scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the 
k nearest neighbors of each query point, where k is an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius r of the query point, where r is a floating-point value specified by the user.
```

As you may have noticed k-Neares Neighbors is within the scikit-learn library. This is the most prominent machine learning library in Python and contains state-of the art machine learning methods! You can find a lot of information on their [website](https://scikit-learn.org/stable/) so make sure to keep that handy! 
Now we will be implementing our classification algorithm! 

### Task 3: 

Following the example detailed in the [scikit website](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py).

Import all functionalities from libraries needed as well as the dataset. First, visualize the iris dataset! What do each of the columns in the data represent?

Then run the code. First plot the boundaries defined by the model without the training data on the plot. Then add the training data to your plot. 

```
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.inspection import DecisionBoundaryDisplay

n_neighbors = 15

# import some data to play with
iris = datasets.load_iris()

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

# Create color maps
cmap_light = ListedColormap(["orange", "cyan", "cornflowerblue"])
cmap_bold = ["darkorange", "c", "darkblue"]

for weights in ["uniform", "distance"]:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    _, ax = plt.subplots()
    DecisionBoundaryDisplay.from_estimator(
        clf,
        X,
        cmap=cmap_light,
        ax=ax,
        response_method="predict",
        plot_method="pcolormesh",
        xlabel=iris.feature_names[0],
        ylabel=iris.feature_names[1],
        shading="auto",
    )

    # Plot also the training points
    sns.scatterplot(
        x=X[:, 0],
        y=X[:, 1],
        hue=iris.target_names[y],
        palette=cmap_bold,
        alpha=1.0,
        edgecolor="black",
    )
    plt.title(
        "3-Class classification (k = %i, weights = '%s')" % (n_neighbors, weights)
    )

plt.show()
```

### This is the end of this training exercise. In order to become proficient with these methodologies you will need PRACTICE! So make sure to look at examples on the scikit-learn website and run the codes yourself!
